{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook should be run outside of the folder containing CAAFE code. The CAAFE code should lie in a directory called 'cafe_feature_engineering' one level above this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install openai\n",
    "!pip install kaggle\n",
    "!pip install openml\n",
    "!pip install submitit\n",
    "!pip install tabpfn[full]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install autofeat\n",
    "!pip install featuretools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HMLKSMsq2EBe",
    "outputId": "e88e017a-9a6d-4df1-da05-5ca6adda3be8"
   },
   "outputs": [],
   "source": [
    "#!ls ~/.kaggle/kaggle.json\n",
    "\n",
    "#!mkdir ~/.kaggle\n",
    "!touch ~/.kaggle/kaggle.json\n",
    "\n",
    "kaggle_api_token = {\"username\":\"XXX\",\"key\":\"XXX\"}\n",
    "\n",
    "import json\n",
    "with open('~/.kaggle/kaggle.json', 'w') as file:\n",
    "    json.dump(kaggle_api_token, file)\n",
    "\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "!mkdir datasets_kaggle/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir {base_path}/results\n",
    "!mkdir {base_path}/results/tabular/\n",
    "!mkdir {base_path}/results/tabular/multiclass/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Download from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cafe_feature_engineering import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (name, _, _, user) in data.kaggle_dataset_ids:\n",
    "    !kaggle datasets download -d {user}/{name}\n",
    "    !mkdir datasets_kaggle/{name}\n",
    "    !unzip {name}.zip -d datasets_kaggle/{name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accept rules at https://www.kaggle.com/c/spaceship-titanic/rules\n",
    "for name in data.kaggle_competition_ids:\n",
    "    print(name)\n",
    "    !kaggle competitions download -c {name}\n",
    "    !mkdir datasets_kaggle/{name}\n",
    "    !unzip {name}.zip -d datasets_kaggle/{name}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TimC16DmEssW"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "\n",
    "from tabpfn.scripts import tabular_baselines\n",
    "\n",
    "import numpy as np\n",
    "from tabpfn.scripts.tabular_baselines import *\n",
    "from tabpfn.scripts.tabular_evaluation import evaluate\n",
    "from tabpfn.scripts import tabular_metrics\n",
    "from tabpfn import TabPFNClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from cafe_feature_engineering import data, cafe, plotting, evaluate, feature_extension_baselines\n",
    "import tabpfn\n",
    "import submitit\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \"XXX\" # You can get an openai access key by creating an account at openai\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "os.environ[\"DATA_DIR\"] = \"cafe_feature_engineering/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kYQ6MLS9Wdjh"
   },
   "outputs": [],
   "source": [
    "metric_used = tabular_metrics.auc_metric\n",
    "methods = ['transformer', 'logistic', 'gp', 'knn', 'catboost', 'xgb', 'autosklearn2', 'autogluon', 'random_forest']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "classifier = TabPFNClassifier(device=\"cpu\", N_ensemble_configurations=16)\n",
    "classifier.fit = partial(classifier.fit,  overwrite_warning=True)\n",
    "tabpfn_method = partial(clf_dict[\"transformer\"], classifier=classifier)\n",
    "\n",
    "classifier_fast = TabPFNClassifier(device=\"cpu\", N_ensemble_configurations=1)\n",
    "classifier_fast.fit = partial(classifier_fast.fit,  overwrite_warning=True)\n",
    "tabpfn_method_fast = partial(clf_dict[\"transformer\"], classifier=classifier_fast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DyNQnECGFVbb",
    "outputId": "a2bfc027-ec36-4a14-c9da-99a3d05f2002",
    "tags": []
   },
   "outputs": [],
   "source": [
    "cc_test_datasets_multiclass = data.load_all_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Test run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = cc_test_datasets_multiclass[5]\n",
    "seed = 1\n",
    "ds, df_train, df_test, df_train_old, df_test_old = data.get_data_split(ds, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "code, prompt, messages = cafe.generate_features(ds,\n",
    "                                                df_train,\n",
    "                                                just_print_prompt=False,\n",
    "                                                model=\"gpt-4\",\n",
    "                                                iterative=10,\n",
    "                                                iterative_method=tabpfn_method,\n",
    "                                                metric_used=metric_used)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Run experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Setup queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_queue = {}\n",
    "global ex\n",
    "global q\n",
    "maximum_runtime = 0\n",
    "log_folder = 'logs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_locally(f, *args, **kwargs):\n",
    "    return f(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Generate Feature Extension Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "jobs = []\n",
    "submit_func = run_locally\n",
    "submit_func = ex.submit\n",
    "def exec_(seed, dsid):\n",
    "    subprocess.run(f'python -m cafe_feature_engineering.generate_features_script --seed {seed} --dataset_id {dsid} --prompt_id v3 --iterations 1', shell=True)\n",
    "    return None\n",
    "    \n",
    "for n in tqdm(range(0, len(cc_test_datasets_multiclass))): # len(cc_test_datasets_multiclass)-1\n",
    "    for seed in range(0, 5):\n",
    "        jobs += [submit_func(exec_, seed, n)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Run Evaluations of Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "jobs = []\n",
    "methods = ['autogluon', 'autosklearn', \"random_forest\", tabpfn_method, \"logistic\"] # ,\"random_forest\", tabpfn_method, \"logistic\", \"autosklearn2\", \"autogluon\"\n",
    "prompts = ['v3', 'v4+dfs', 'autofeat', 'v4+autofeat', 'v3+autofeat', 'v3+dfs']\n",
    "submit_func = run_locally\n",
    "for method in methods[::-1]:\n",
    "    for prompt_id in prompts:\n",
    "        for n in tqdm(range(0, len(cc_test_datasets_multiclass))): # len(cc_test_datasets_multiclass)-1\n",
    "            for seed in range(0, 5):\n",
    "                ds = cc_test_datasets_multiclass[n]\n",
    "                method_str = method if type(method) == str else \"transformer\"\n",
    "                data_dir = os.environ.get(\"DATA_DIR\", \"data/\")\n",
    "                path = (\n",
    "                    f\"{data_dir}/evaluations/result_{ds[0]}_{prompt_id}_{seed}_{method_str}.txt\"\n",
    "                )\n",
    "                #if os.path.exists(path):\n",
    "                #    continue\n",
    "                #else:\n",
    "                #    print('no exist')\n",
    "                jobs += [submit_func(evaluate.evaluate_dataset_with_and_without_cafe, ds,\n",
    "                                                     seed,\n",
    "                                                     [method],\n",
    "                                                    metric_used,\n",
    "                                     overwrite=True,\n",
    "                                                    prompt_id=prompt_id\n",
    "                )]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Visualize results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = {}\n",
    "all_prompts = ['', 'v4', 'v3', 'dfs', 'v4+dfs', 'autofeat', 'v4+autofeat']\n",
    "all_methods = [tabpfn_method, \"random_forest\", \"logistic\", \"autosklearn\", \"autogluon\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for prompt_id in all_prompts:\n",
    "    for method in all_methods: # tabpfn, \"logistic\",  \"logistic\", \"random_forest\", \n",
    "        method_str = method if type(method) == str else \"transformer\"\n",
    "        for n in tqdm(range(0, len(cc_test_datasets_multiclass))): # len(cc_test_datasets_multiclass)-1\n",
    "            for seed in range(0, 5):\n",
    "                ds = cc_test_datasets_multiclass[n]\n",
    "                r = evaluate.load_result(all_results, cc_test_datasets_multiclass[n],seed,method,prompt_id=prompt_id)\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_relabeler = {'transformer': 'Tabular PFN'\n",
    "             , 'autogluon': 'Autogluon'\n",
    "             , 'autosklearn2': 'Autosklearn2'\n",
    "             , 'ridge':'Ridge'\n",
    "             , 'gp': 'GP (RBF)'\n",
    "             , 'bayes': 'BNN'\n",
    "             , 'tabnet': 'Tabnet'\n",
    "             , 'logistic': 'Log. Regr.'\n",
    "             , 'knn': 'KNN'\n",
    "             , 'catboost': 'Catboost'\n",
    "            , 'xgb': 'XGB'}\n",
    "\n",
    "def rename_table_vis(table):\n",
    "    ren = {'blood-transfusion-service-center': 'blood-transfus..',\n",
    "        'jungle_chess_2pcs_raw_endgame_complete': 'jungle_chess..',\n",
    "       'bank-marketing': 'bank-market..',\n",
    "       'kaggle_spaceship-titanic': '[Kaggle] spaceship-titanic',\n",
    "       'kaggle_playground-series-s3e12': '[Kaggle] kidney-stone',\n",
    "       'kaggle_health-insurance-lead-prediction-raw-data': '[Kaggle] health-insurance',\n",
    "       'kaggle_pharyngitis': '[Kaggle] pharyngitis'\n",
    "       \n",
    "      }\n",
    "    \n",
    "    return table.rename(columns=clf_relabeler).T.rename(columns=ren).T\n",
    "\n",
    "def table_sorter(x):\n",
    "    methods_sort = {'logistic': 0, 'random_forest': 1, 'autogluon': 2, 'autosklearn': 3, 'transformer': 4}\n",
    "    prompts_sort = {'': 0, 'dfs': 1, 'v4+dfs': 2, 'autofeat': 3, 'v4+autofeat': 4, 'v3': 5, 'v4': 6}\n",
    "    x = x.split('_')\n",
    "    return str(methods_sort.get(x[0], 9)) + str(prompts_sort.get(x[1], 9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Creating dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = 'roc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.DataFrame(all_results).T\n",
    "df_all = df_all.set_index('name')\n",
    "\n",
    "# Filtering\n",
    "df_all = df_all[df_all.seed < 5]\n",
    "df_all = df_all[df_all.index != \"wine\"]\n",
    "df_all = df_all[(df_all.method != \"autogluon\") & (df_all.method != \"autosklearn\")]\n",
    "\n",
    "# How many features added?\n",
    "feats_extended = df_all[np.logical_and.reduce((df_all.prompt == prompt_id, df_all.seed == 0, df_all.method == 'logistic'))].feats.sum()\n",
    "feats_old = df_all[np.logical_and.reduce((df_all.prompt == '', df_all.seed == 0, df_all.method == 'logistic'))].feats.sum()\n",
    "print('Features added', feats_extended, feats_old)\n",
    "\n",
    "# Create results dataframe\n",
    "df_all_agg_seeds = df_all.groupby(by=[\"name\", \"method\", \"prompt\"])['acc'].mean()\n",
    "rank_df = df_all_agg_seeds.groupby(by=[\"name\", \"method\"]).rank(ascending=False)\n",
    "\n",
    "df_all['rank_within_ds'] = rank_df\n",
    "df_all['wins_within_ds'] = rank_df == 1.0\n",
    "df_all['ties_within_ds'] = rank_df == 1.5\n",
    "\n",
    "df_all_grouped_by_method = df_all.groupby(by=[\"prompt\", \"method\"]).agg({'acc': ['mean'],\n",
    "                                   'roc': ['mean'],\n",
    "                                   'rank_within_ds': ['mean'],\n",
    "                                   'wins_within_ds': ['sum'],\n",
    "                                  'ties_within_ds': ['sum']\n",
    "                                                                       }).T\n",
    "\n",
    "df_all_grouped_by_ds = df_all.groupby(by=[\"name\", \"prompt\", \"method\", \"seed\"]).agg({'acc': ['mean'], 'roc': ['mean']})\n",
    "df_all_grouped_by_ds.columns = df_all_grouped_by_ds.columns.get_level_values(0)\n",
    "df_all_grouped_by_ds = df_all_grouped_by_ds.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Print table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Check all results are ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_grouped_by_ds_table_1 = df_all_grouped_by_ds.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_grouped_by_ds_print = df_all_grouped_by_ds_table_1.groupby(by=[\"name\", \n",
    "                                                              \"prompt\", \"method\"])[metric].count().reset_index().pivot(index='name', columns=['method', 'prompt'], values=metric)\n",
    "df_all_grouped_by_ds_print.columns = ['_'.join(col) for col in df_all_grouped_by_ds_print.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 500)\n",
    "df_all_grouped_by_ds_print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Table only CAFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_grouped_by_ds_table_1 = df_all_grouped_by_ds.copy()\n",
    "df_all_grouped_by_ds_table_1 = df_all_grouped_by_ds_table_1[(df_all_grouped_by_ds.method== \"transformer\")]\n",
    "df_all_grouped_by_ds_table_1 = df_all_grouped_by_ds_table_1[(df_all_grouped_by_ds_table_1.prompt == \"v4\") | (df_all_grouped_by_ds_table_1.prompt == \"\") | (df_all_grouped_by_ds_table_1.prompt == \"v3\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_grouped_by_ds_print = df_all_grouped_by_ds_table_1.groupby(by=[\"name\", \"prompt\", \"method\"])[metric].mean().reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stds = df_all_grouped_by_ds_table_1.groupby(by=[\"name\", \"prompt\", \"method\"])[metric].std().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_grouped_by_ds_print['ranks'] = df_all_grouped_by_ds_print.groupby(['name', 'method']).rank()['roc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_grouped_by_ds_print_ranks = df_all_grouped_by_ds_print.copy().pivot(index='name', columns=['method', 'prompt'], values='ranks')\n",
    "df_all_grouped_by_ds_print_ranks.columns = ['_'.join(col) for col in df_all_grouped_by_ds_print_ranks.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_grouped_by_ds_print_ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_grouped_by_ds_print = df_all_grouped_by_ds_print.pivot(index='name', columns=['method', 'prompt'], values=metric)\n",
    "df_all_grouped_by_ds_print.columns = ['_'.join(col) for col in df_all_grouped_by_ds_print.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_grouped_by_ds_print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_grouped_by_ds_print_stds = stds.pivot(index='name', columns=['method', 'prompt'], values=metric)\n",
    "df_all_grouped_by_ds_print_stds.columns = ['_'.join(col) for col in df_all_grouped_by_ds_print_stds.columns]\n",
    "df_all_grouped_by_ds_print_stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_grouped_by_ds_print.loc[f'Mean ROC'] = df_all_grouped_by_ds_print.mean(axis=1,level=0).mean().values\n",
    "df_all_grouped_by_ds_print.loc[f'Mean ROC Stds'] = df_all_grouped_by_ds_print_stds.mean(axis=1,level=0).mean().values\n",
    "df_all_grouped_by_ds_print.loc[f'Mean Rank'] = df_all_grouped_by_ds_print_ranks.mean(axis=1,level=0).mean().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = df_all_grouped_by_ds_print.columns.tolist()\n",
    "cols = sorted(cols)\n",
    "N_end = 0\n",
    "N_cols = 10\n",
    "N_methods = len(all_methods) - 2\n",
    "offset = 0\n",
    "#cols = [cols[i // N_cols + (i % N_cols) * N_methods] for i in range(0, len(cols) - N_end)]# + cols[-N_end:]\n",
    "df_all_grouped_by_ds_print = df_all_grouped_by_ds_print[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bold_extreme_values(data, format_string=\"%.5g\", max_=True):\n",
    "    data = data.astype(float).round(4)\n",
    "    if max_:\n",
    "        extrema = data != data.max()\n",
    "    else:\n",
    "        extrema = data != data.min()\n",
    "    bolded = data.apply(lambda x : \"\\\\textbf{%s}\" % format_string % x)\n",
    "    formatted = data.apply(lambda x : format_string % x)\n",
    "    return formatted.where(extrema, bolded) \n",
    "\n",
    "def to_str(data, format_string=\"%.3g\", drop=False):\n",
    "    if drop:\n",
    "        formatted = data.apply(lambda x : (format_string % x)[1:])\n",
    "    else:\n",
    "        formatted = data.apply(lambda x : (format_string % x))\n",
    "    return formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table.index[:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = rename_table_vis(df_all_grouped_by_ds_print).copy()\n",
    "\n",
    "non_agg = table.index[:-3]\n",
    "table.loc[non_agg] = table.loc[non_agg].apply(lambda data : bold_extreme_values(data),axis=1)\n",
    "table.loc[non_agg] =  table.loc[non_agg] + ' {\\\\scriptsize $\\pm$' +  rename_table_vis(df_all_grouped_by_ds_print_stds).loc[non_agg].apply(lambda data : to_str(data, format_string=\"%.2f\", drop=True),axis=1) + '}'\n",
    "\n",
    "table.loc[['Mean ROC']] = table.loc[['Mean ROC']].apply(lambda data : bold_extreme_values(data), axis=1)\n",
    "table.loc[['Mean ROC Stds']] = table.loc[['Mean ROC Stds']].apply(lambda data : to_str(data, format_string=\"%.2f\", drop=True),axis=1)\n",
    "table.loc['Mean ROC'] = table.loc['Mean ROC'] + ' {\\\\scriptsize $\\pm$' + table.loc['Mean ROC Stds'] +'}'\n",
    "table = table.drop(['Mean ROC Stds'])\n",
    "\n",
    "table.loc[['Mean Rank']] = table.loc[['Mean Rank']].apply(lambda data : bold_extreme_values(data, format_string=\"%.2f\"), axis=1)\n",
    "\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "tab_string = table.to_latex(escape=False).replace('[Kaggle]', '$\\\\langle Kaggle\\\\rangle$')\n",
    "tab_string = re.sub(r' \\\\font-weightbold ([0-9\\.]*) ', ' \\\\\\\\textbf{\\\\1} ', tab_string)\n",
    "tab_string = tab_string.replace(r\"\"\"\\begin{tabular}{llll}\n",
    "\\toprule\n",
    "{} &             transformer_ &           transformer_v3 &           transformer_v4 \\\\\n",
    "name                       &                          &                          &                          \\\\\"\"\", r\"\"\"\\begin{tabular}{l|r|r|rr}\n",
    "\\toprule\n",
    "{} & \\multicolumn{1}{c}{TabPFN} & \\multicolumn{1}{c}{TabPFN + CAAFE (GPT-3.5)} & \\multicolumn{1}{c}{TabPFN + CAAFE (GPT-4)} \\\\\"\"\")\n",
    "print(tab_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Table 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_grouped_by_ds_table_1 = df_all_grouped_by_ds.copy()\n",
    "#df_all_grouped_by_ds_table_1 = df_all_grouped_by_ds_table_1[df_all_grouped_by_ds_table_1.prompt != \"v3\"]\n",
    "#df_all_grouped_by_ds_table_1 = df_all_grouped_by_ds_table_1[np.logical_or(df_all_grouped_by_ds_table_1.prompt == \"\", df_all_grouped_by_ds_table_1.method == \"transformer\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_grouped_by_ds_table_1['ranks'] = df_all_grouped_by_ds_table_1.groupby(by=[\"prompt\", \"method\"])[metric].rank()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_grouped_by_ds_table_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_grouped_by_ds_print = df_all_grouped_by_ds_table_1.groupby(by=[\"prompt\", \"method\"])[metric].mean().reset_index()\n",
    "#df_all_grouped_by_ds_print.columns = ['_'.join(col) for col in df_all_grouped_by_ds_print.columns]\n",
    "\n",
    "#df_all_grouped_by_ds_print.loc[f'Mean ROC'] = df_all_grouped_by_ds_print.mean(axis=1,level=0).mean().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_grouped_by_ds_print = df_all_grouped_by_ds_print.reset_index().pivot(index='method', columns=['prompt'], values=metric)\n",
    "df_all_grouped_by_ds_print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stds = df_all_grouped_by_ds_table_1.groupby(by=[\"prompt\", \"method\"])[metric].std().reset_index()\n",
    "df_all_grouped_by_ds_print_stds = stds.reset_index().pivot(index='method', columns=['prompt'], values=metric)\n",
    "df_all_grouped_by_ds_print_stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = df_all_grouped_by_ds_print.reindex([\"logistic\", \"random_forest\", \"transformer\"]).copy()\n",
    "\n",
    "non_agg = table.index\n",
    "table.loc[non_agg] = table.loc[non_agg].apply(lambda data : bold_extreme_values(data, format_string=\"%.3g\"),axis=1)\n",
    "table.loc[non_agg] =  table.loc[non_agg] + ' {\\\\scriptsize $\\pm$' + rename_table_vis(df_all_grouped_by_ds_print_stds).loc[non_agg].apply(lambda data : to_str(data, format_string=\"%.2f\", drop=True),axis=1)+ '}'\n",
    "\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "tab_string = table.to_latex(escape=False).replace('[Kaggle]', '$\\\\langle Kaggle\\\\rangle$')\n",
    "tab_string = re.sub(r' \\\\font-weightbold ([0-9\\.]*) ', ' \\\\\\\\textbf{\\\\1} ', tab_string)\n",
    "tab_string = tab_string.replace(\"transformer\", \"TabPFN\")\n",
    "tab_string = tab_string.replace(\"logistic\", \"Log. Reg.\")\n",
    "tab_string = tab_string.replace(\"random_forest\", \"Random Forest\")\n",
    "tab_string = tab_string.replace(r\"\"\"\\begin{tabular}{llllllll}\n",
    "\\toprule\n",
    "prompt &        autofeat &             dfs &              v3 &                       v4 &             v4+autofeat &                   v4+dfs \\\\\n",
    "method        &                 &                 &                 &                 &                          &                         &                          \\\\\"\"\"\n",
    "                                , r\"\"\"\\begin{tabular}{llllllll}\n",
    "\\toprule\n",
    "{} & \\multicolumn{1}{c}{} & \\multicolumn{2}{c}{Baselines} & \\multicolumn{2}{c}{CAAFE} & \\multicolumn{2}{c}{Baseline + CAAFE} \\\\\"\"\")\n",
    "print(tab_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Table 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This table only shows TabPFN performance and then different extension baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_grouped_by_ds_table_1 = df_all_grouped_by_ds.copy()\n",
    "#df_all_grouped_by_ds_table_1 = df_all_grouped_by_ds_table_1[df_all_grouped_by_ds_table_1.prompt != \"v3\"]\n",
    "#df_all_grouped_by_ds_table_1 = df_all_grouped_by_ds_table_1[np.logical_or(df_all_grouped_by_ds_table_1.prompt == \"\", df_all_grouped_by_ds_table_1.method == \"transformer\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_all_grouped_by_ds_print = df_all_grouped_by_ds_table_1.groupby(by=[\"name\", \n",
    "                                                              \"prompt\", \"method\"])[metric].mean().reset_index().pivot(index='name', columns=['method', 'prompt'], values=metric)\n",
    "df_all_grouped_by_ds_print.columns = ['_'.join(col) for col in df_all_grouped_by_ds_print.columns]\n",
    "\n",
    "df_all_grouped_by_ds_print.loc[f'Mean ROC'] = df_all_grouped_by_ds_print.mean(axis=1,level=0).mean().values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = df_all_grouped_by_ds_print.columns.tolist()\n",
    "cols = sorted(cols, key=table_sorter)\n",
    "N_end = 0\n",
    "N_cols = len(all_prompts)\n",
    "N_methods = len(all_methods)\n",
    "offset = 0\n",
    "#cols = [cols[i // N_cols + (i % N_cols) * N_methods] for i in range(0, len(cols) - N_end)]# + cols[-N_end:]\n",
    "df_all_grouped_by_ds_print = df_all_grouped_by_ds_print[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(offset, len(df_all_grouped_by_ds_print.columns)):\n",
    "    if (i - offset) % N_cols == 0:\n",
    "        continue\n",
    "    comparison_idx = offset + N_cols * ((i - offset) // N_cols)\n",
    "    df_all_grouped_by_ds_print.iloc[:, i] = df_all_grouped_by_ds_print.iloc[:, i] - df_all_grouped_by_ds_print.iloc[:, comparison_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = (rename_table_vis(df_all_grouped_by_ds_print).round(decimals=4)\n",
    "         .style\n",
    "         .highlight_max(subset=df_all_grouped_by_ds_print.columns[offset+1:offset+N_cols], axis=1, props='font-weight: bold;')\n",
    "         .highlight_max(subset=df_all_grouped_by_ds_print.columns[offset+N_cols+1:offset+N_cols*2], axis=1, props='font-weight: bold;')\n",
    "         .highlight_max(subset=df_all_grouped_by_ds_print.columns[offset+N_cols*2+1:offset+N_cols*3], axis=1, props='font-weight: bold;')\n",
    "         .format(precision=4))\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "tab_string = table.to_latex().replace('[Kaggle]', '$\\\\langle Kaggle\\\\rangle$')\n",
    "tab_string = re.sub(r' \\\\font-weightbold ([0-9\\.]*) ', ' \\\\\\\\textbf{\\\\1} ', tab_string)\n",
    "print(tab_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Print overview of datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Printing all dataset descriptions\n",
    "for n in tqdm(range(0, len(cc_test_datasets_multiclass))):\n",
    "    ds = cc_test_datasets_multiclass[n]\n",
    "    import re\n",
    "    print(\"\"\"\\\\begin{figure}[h]\n",
    "    \\\\centering\n",
    "    \\\\begin{minipage}{\\\\textwidth}\n",
    "    \\\\begin{lstlisting}\"\"\")\n",
    "    print(data.get_data_split(ds, 0)[0][-1])\n",
    "    print(\"\"\"\\\\end{lstlisting}\n",
    "    \\\\end{minipage}\n",
    "    \\\\caption{Dataset description for \"\"\"+re.escape(ds[0])+\"\"\".}\n",
    "    \\\\label{fig:llm_prompt}\n",
    "\\\\end{figure}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = [{'Name': ds[0], '# Features': ds[1].shape[1], '# Samples': ds[1].shape[0], '# Classes': len(np.unique(ds[2]))\n",
    "      , 'OpenML ID / Kaggle Name': cc_test_datasets_multiclass_df.iloc[i].did if i < len(cc_test_datasets_multiclass_df) else ''} for i, ds in enumerate(cc_test_datasets_multiclass)]\n",
    "print(pd.DataFrame(df).set_index('Name').to_latex())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Create a stripplot of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = 'transformer'\n",
    "metric = 'roc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_ds = (df_all_grouped_by_ds[df_all_grouped_by_ds.method == method].groupby(by=[\"name\", \"prompt\", \"method\"]).agg({metric: ['mean']})\n",
    " .groupby(by=[\"name\", \"method\"])).diff().groupby(by=[\"name\"]).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_grouped_by_ds['diff'] = df_all_grouped_by_ds.apply(lambda x : diff_ds.loc[x['name']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_grouped_by_ds = df_all_grouped_by_ds.sort_values(by=['diff'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_grouped_by_ds[df_all_grouped_by_ds.prompt == \"\"].prompt = \"none\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ren = {'blood-transfusion-service-center': 'blood-transfus..',\n",
    "        'jungle_chess_2pcs_raw_endgame_complete': 'jungle_chess..',\n",
    "       'bank-marketing': 'bank-market..',\n",
    "       'kaggle_spaceship-titanic': '[Kaggle] spaceship-titanic',\n",
    "       'kaggle_playground-series-s3e12': '[Kaggle] kidney-stone',\n",
    "       'kaggle_health-insurance-lead-prediction-raw-data': '[Kaggle] health-insurance',\n",
    "       'kaggle_pharyngitis': '[Kaggle] pharyngitis'\n",
    "       \n",
    "      }\n",
    "df_all_grouped_by_ds.name = df_all_grouped_by_ds.name.apply(lambda x : ren[x] if x in ren else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plotting.draw_stripplot(\n",
    "    df_all_grouped_by_ds[df_all_grouped_by_ds.method == method], x=metric, y=\"name\", hue=\"prompt\", size=(15, 6)\n",
    "    , xbound=[0.5, 1.05]\n",
    "#, legend_labels=['Using CAFE', 'Using DFS', ]\n",
    ", legend_title=' '\n",
    ", legend_loc='upper left')\n",
    "plt.subplots_adjust(left=0.2, right=1.0, top=1.0, bottom=0.0)\n",
    "import tikzplotlib\n",
    "plt.savefig(f\"results_{method}_{metric}.pdf\")\n",
    "#tikzplotlib.save(f\"results_{method}_{metric}.tex\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
